cuda_visible_devices: "0"
device: gpu

dataset_name: BBC
logger_name: df_logger_cw_BBC.csv
model_name: Extended_NoTemp_w30      # Options: MHA_ReLu, MHA_Anneal, MHA_Sigmoid
setting_file: /path/to/config/file/of/the/corresponding/MHA-Model/
type_model: GAT
binarized: False
multi_layer: False                              # True for multi-layer attention models, False for single-layer
baseline: False                                 # Always False for attention-based learned graphs
type_graph: full                                # "full", "max" or "mean", depending on the type of statistical filtering to use
unified_nodes: True


data_paths:
  path_logger: /content/drive/MyDrive/ADG/HomoGraphs_BBC/
  results_folder: /content/drive/MyDrive/ADG/AttGraphs/GNN_Results_Classifier/Colab/
  root_graph_dataset: /content/drive/MyDrive/ADG/AttnGraphs_BBC/

load_data_paths:
  in_path: "/content/drive/MyDrive/Colab Notebooks/datasets/BBC/"
  data_train: ""
  labels_train: ""
  data_test: ""
  labels_test: ""
  with_val: True                               # True if validation set is available, False otherwise (like this case)

model_arch_args:
  num_classes: 5
  lr: 0.001
  dropout: 0.2
  dim_features : [64, 128, 256]
  n_layers: [1, 2, 3]
  num_runs: 5
  window: 30                              # set to 100 to run full self-attention, otherwise 1-99 according to % of document length

batch_size: 32
with_cw: True

trainer_args:
  max_epochs: 50
  enable_progress_bar: False

early_args:
  patience: 5
  min_delta: 0.001